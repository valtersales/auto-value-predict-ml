# AutoValuePredict ML

AutoValuePredict is a machine learning project designed to predict the market value of used cars in Brazil. It demonstrates end-to-end ML development, including data collection, cleaning, feature engineering, regression modeling, model evaluation, and deployment.

This project aims to serve as a practical and professional showcase of data science and machine learning skills, while also producing a reusable prediction pipeline and a simple API for real-time price estimation.

## Project Structure

```
auto-value-predict-ml/
â”œâ”€â”€ .dockerignore          # Docker ignore patterns
â”œâ”€â”€ .gitignore            # Git ignore patterns
â”œâ”€â”€ Dockerfile            # Docker image configuration
â”œâ”€â”€ docker-compose.yml    # Docker Compose configuration
â”œâ”€â”€ Makefile              # Convenient commands for Docker operations
â”œâ”€â”€ pyproject.toml        # Poetry dependency management
â”œâ”€â”€ requirements.txt      # pip dependency management (alternative)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Raw data files
â”‚   â””â”€â”€ processed/       # Processed/cleaned data files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/            # Data collection and loading modules
â”‚   â”œâ”€â”€ features/        # Feature engineering modules
â”‚   â”œâ”€â”€ models/          # Model training and evaluation modules
â”‚   â””â”€â”€ api/             # FastAPI application for serving predictions
â”œâ”€â”€ notebooks/           # Jupyter notebooks for exploratory analysis
â”œâ”€â”€ models/              # Trained model artifacts
â””â”€â”€ tests/               # Unit and integration tests
```

## Technologies

### Languages & Core

- **Python 3.10+**

### Data & ML

- **Pandas** â€” data manipulation
- **NumPy** â€” numerical operations
- **SciPy** â€” statistical functions (skewness, kurtosis, Z-score)
- **scikit-learn** â€” ML algorithms, preprocessing, pipelines
- **Matplotlib / Seaborn** â€” data visualization (exploratory analysis)
- **XGBoost / LightGBM** â€” boosted models for high accuracy

### API & Deployment

- **FastAPI** â€” REST API for serving predictions
- **Uvicorn** â€” application server
- **Docker** â€” containerization for portability
- **Poetry / pip** â€” dependency management

## Getting Started

### Prerequisites

- Docker and Docker Compose installed
- (Optional) Python 3.10+ and Poetry/pip for local development

### Using Docker (Recommended)

1. **Build the Docker image:**

   ```bash
   make build
   ```

2. **Start the containers:**

   ```bash
   make up
   ```

3. **View logs:**

   ```bash
   make logs
   ```

4. **Stop the containers:**
   ```bash
   make down
   ```

### Available Make Commands

Run `make help` to see all available commands:

**Container Management:**

- `make build` - Build the Docker image
- `make up` - Start the containers
- `make down` - Stop the containers
- `make restart` - Restart the containers
- `make logs` - Show container logs
- `make shell` - Open a shell in the running container
- `make status` - Show container status

**Jupyter Notebook:**

- `make jupyter` - Start Jupyter Lab server (accessible at http://localhost:8888)
- `make logs-jupyter` - Show Jupyter container logs
- `make stop-jupyter` - Stop Jupyter container

**Cleanup:**

- `make clean` - Stop and remove containers, networks
- `make clean-containers` - Remove all stopped containers
- `make clean-images` - Remove unused Docker images
- `make clean-volumes` - Remove unused volumes
- `make clean-all` - Remove containers, images, and volumes (WARNING: destructive)

**Shortcuts:**

- `make rebuild` - Clean and rebuild the Docker image
- `make start` - Build and start the containers
- `make stop` - Stop the containers

### Local Development

If you prefer to run locally without Docker:

1. **Install dependencies using Poetry:**

   ```bash
   poetry install
   ```

   Or using pip:

   ```bash
   pip install -r requirements.txt
   ```

2. **Activate the virtual environment:**
   ```bash
   poetry shell
   ```

## Dataset

The project uses enriched FIPE (FundaÃ§Ã£o Instituto de Pesquisas EconÃ´micas) data for predicting used car prices in Brazil.

### Data Source

The raw data was downloaded from Kaggle:

- **Source**: [Average Car Prices Brazil](https://www.kaggle.com/datasets/vagnerbessa/average-car-prices-bazil?resource=download)
- **Author**: Vagner Bessa
- **Description**: Historical FIPE table data with average car prices in Brazil

### Data Enrichment

For educational purposes, the raw FIPE data has been enriched with synthetic but realistic values for missing features:

- **Quilometragem (km)**: Calculated based on vehicle age and price, following realistic usage patterns (average 15,000 km/year)
- **Location (state, city)**: Assigned based on Brazilian population distribution
- **Color**: Common car colors in Brazil (White, Silver, Black, etc.)
- **Doors**: Number of doors (2, 4, or 5)
- **Condition**: Vehicle condition (Regular, Good, Great, Excellent) based on age and mileage

The enrichment script (`src/data/enrich_fipe_data.py`) uses statistical patterns to generate realistic synthetic data that maintains the characteristics of the Brazilian used car market.

> **âš ï¸ Important**: These enriched features are synthetically generated for educational purposes. While they follow realistic patterns, they are not real-world observations. Model predictions should be validated against actual market data before any production use.

### Dataset Files

- **Raw data** (`data/raw/`):

  - `fipe_cars.csv` - Historical FIPE data (599,007 records)
  - `fipe_2022.csv` - 2022 FIPE data subset (290,275 records)

- **Processed data** (`data/processed/`):
  - `fipe_cars_enriched.csv` - Enriched historical data with all features
  - `fipe_2022_enriched.csv` - Enriched 2022 data with all features

### Data Schema

The enriched datasets include the following columns:

- `brand`, `model`, `year` - Vehicle identification
- `price` - Target variable (price in Brazilian Reais)
- `km` - Mileage in kilometers
- `state`, `city` - Location
- `fuel_type`, `transmission`, `engine_size` - Technical specifications
- `color`, `doors`, `condition` - Additional features
- `age_years` - Vehicle age

## Project Status

ğŸš§ **In Development** - Current progress:

- âœ… Project structure and Docker configuration
- âœ… Data collection and enrichment (599k+ records)
- âœ… Exploratory Data Analysis (EDA) - Completed
  - âœ… Phase 1.1: Initial Data Exploration (`01_data_overview.ipynb`)
  - âœ… Phase 1.2: Target Variable Analysis (`02_target_analysis.ipynb`)
  - âœ… Phase 1.3: Feature Analysis (`03_feature_analysis.ipynb`)
  - âœ… Phase 1.4: Relationships and Correlations (`04_correlations.ipynb`)
  - âœ… Phase 1.5: Data Quality Assessment (`05_data_quality.ipynb`)
- âœ… Data preprocessing and cleaning - Completed
  - âœ… Data cleaning pipeline (`src/data/cleaner.py`)
  - âœ… Data validation (`src/data/validator.py`)
  - âœ… Data splitting (`src/data/splitter.py`)
  - âœ… Modular ML pipeline system (`src/pipeline/`)
  - âœ… Unit tests for all modules
  - âœ… Pipeline execution scripts and Makefile commands
  - âœ… Processed 747,948 rows, split into train/val/test (70/15/15)
- âœ… Feature engineering - Completed
  - âœ… Feature engineering pipeline (`src/features/pipeline.py`)
  - âœ… Feature engineering modules (`src/features/engineering.py`, `src/features/selectors.py`)
  - âœ… Temporal features, categorical encoding, numerical transformations
  - âœ… Location features and advanced features (optional)
  - âœ… FeatureEngineeringStep integrated into main pipeline
- âœ… Baseline models - Completed
  - âœ… Baseline model implementations (`src/models/baseline.py`)
  - âœ… Evaluation metrics module (`src/models/evaluator.py`)
  - âœ… Mean/Median, Linear Regression, Ridge, Lasso, Decision Tree models
  - âœ… TrainBaselineModelsStep integrated into main pipeline
  - âœ… Training script (`scripts/train_baseline_models.py`)
  - âœ… Results saved to `models/baseline_results/` (metrics, reports, plots)
- âœ… Advanced model development - Completed
  - âœ… Advanced model trainer (`src/models/trainer.py`)
  - âœ… Random Forest with RandomizedSearchCV (2-fold CV, optimized for memory)
  - âœ… XGBoost with validation-based hyperparameter search
  - âœ… LightGBM with validation-based hyperparameter search
  - âœ… TrainAdvancedModelsStep integrated into main pipeline
  - âœ… Training script (`scripts/train_advanced_models.py`)
  - âœ… Results saved to `models/advanced_results/` (metrics, reports, plots, models)
- âœ… Model optimization and fine-tuning - Completed
  - âœ… Test set evaluation (`scripts/evaluate_test_set.py`)
  - âœ… Hyperparameter optimization (`scripts/optimize_lightgbm.py`)
  - âœ… Segment and error analysis (`scripts/analyze_segments_and_errors.py`)
  - âœ… EvaluateTestSetStep and AnalyzeSegmentsAndErrorsStep integrated into pipeline
- âœ… Model persistence and versioning - Completed
  - âœ… Model persistence module (`src/models/persistence.py`)
  - âœ… Model versioning system with registry
  - âœ… SaveModelWithVersioningStep integrated into pipeline
  - âœ… Model saving and loading scripts
- ğŸš§ API implementation - Next
- â³ Model deployment

**Development Strategy**: MVP-first approach - building essential features for a working end-to-end pipeline, then iterating with enhancements.

**Estimated Timeline**:

- MVP (Essential features): 10-12 weeks
- Full Implementation: 12-16 weeks

See [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md) for detailed roadmap and task breakdown.

## ML Pipeline System

O projeto utiliza um sistema modular de pipeline que permite executar todas as etapas de forma orquestrada e incremental. O pipeline Ã© expandido conforme novas fases sÃ£o implementadas.

### Executar Pipeline

**Via Makefile (recomendado - executa no Docker):**

```bash
# Iniciar containers Docker primeiro
make up

# Executar pipeline completo (atÃ© fases implementadas)
make pipeline

# Listar etapas disponÃ­veis
make pipeline-list

# Ver status do pipeline
make pipeline-status

# Executar apenas Phase 2 (preprocessing)
make pipeline-phase2

# Executar etapa especÃ­fica
make pipeline-step STEP=clean_data

# Executar a partir de uma etapa
make pipeline-from STEP=split_data

# Treinar modelos baseline (standalone)
make train-baseline

# Treinar modelos avanÃ§ados (standalone)
make train-advanced

# Executar testes
make test
make test-cleaner
make test-validator
make test-splitter
```

**Via scripts Python diretamente:**

```bash
# Executar pipeline completo (atÃ© fases implementadas)
python scripts/run_pipeline.py

# Listar etapas disponÃ­veis
python scripts/run_pipeline.py --list-steps

# Ver status do pipeline
python scripts/run_pipeline.py --status

# Executar apenas Phase 2 (preprocessing)
python scripts/preprocess_data.py

# Executar etapas especÃ­ficas
python scripts/run_pipeline.py --start-from clean_data --stop-at split_data

# Treinar modelos baseline (standalone)
python scripts/train_baseline_models.py

# Treinar modelos avanÃ§ados (standalone)
python scripts/train_advanced_models.py
```

### Estrutura do Pipeline

O pipeline Ã© composto por etapas (`PipelineStep`) que sÃ£o executadas sequencialmente:

1. **LoadDataStep**: Carrega datasets enriquecidos
2. **ValidateDataStep**: Valida qualidade dos dados
3. **CleanDataStep**: Limpa e preprocessa dados
4. **SplitDataStep**: Divide dados em train/val/test
5. **FeatureEngineeringStep**: Engenharia de features (Phase 3)
6. **TrainBaselineModelsStep**: Treina modelos baseline (Phase 4)
7. **TrainAdvancedModelsStep**: Treina modelos avanÃ§ados (Phase 5) âœ…

Cada etapa:
- Valida seus prÃ©-requisitos
- Executa sua lÃ³gica
- Atualiza o contexto compartilhado
- Salva estado para permitir retomar execuÃ§Ã£o

Para mais detalhes, veja [src/pipeline/README.md](src/pipeline/README.md).

## License

See [LICENSE](LICENSE) file for details.
